# -*- coding: utf-8 -*-
"""tokenizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NHsdd0zIKA3w9LhxXNR_r2Y65y1uhQoc
"""


import sentencepiece as spm
import logging
import argparse

LOGGER=logging.getLogger()


def get_argument():
  parser=argparse.ArgumentParser()

  parser.add_argument('--vocab_size', type=int, default=16000)
  parser.add_argument('--datapath',default='./datasets/iwslt14.de.en')
  parser.add_argument('--srcpath',default='./datasets/iwslt14.de.en/train.de-en.de')
  parser.add_argument('--trgpath',default='./datasets/iwslt14.de.en/train.de-en.en')
  parser.add_argument('--langpair', default='de-en')

  return parser.parse_args()

class SentencePieceTokenizer(object):
  def __init__(self,datapath, vocab_size=0,alpha=0,n=0,l=0):
    self.templates='--input={} --model_prefix={} --vocab_size={} \
                    --pad_id=0 --bos_id=2 --eos_id=3 --unk_id=1'                #일단 tokenizer model의 template만들기
    self.vocab_size=vocab_size
    self.spmodel_path="{}/sp".format(datapath)

    #self.alpha=alpha                                                            #???
    #self.n=n
    #self.l=l

  def fit(self,train_input_file):                                                         #init에서 만든 template로 model 만들고 train한다
    cmd=self.templates.format(train_input_file,self.spmodel_path,self.vocab_size,0)
    spm.SentencePieceTrainer.Train(cmd)                                                   #cmd로 template 정함. 여기에는 train에 이용할 파일 들어감. trainer로 이 파일 input으로 하여 tokenizer train한다. 이 과정 끝나면 path에 .model과 .vocab파일 생김
    

    

  def loadmodel(self):
    file=self.spmodel_path+".model"
    self.sp=spm.SentencePieceProcessor()
    self.sp.Load(file)                                                       #여기까진 model을 load하는 과정이다.
    self.sp.SetEncodeExtraOptions('eos')                                 #token의 끝에 eos를 붙여준다. self.sp= ...으로 작성하면 안 됨
    return self

  def Encode(self,sentence, max_length=0):                                      #단어를 받아서 인코딩 정수로 리턴해준다
   

    
    output=self.sp.EncodeAsIds(sentence)

    if max_length>0:                                                            ###????
        padding=[0]*max_length
        padding[:min(max_length,len(output))]=output[:min(max_length,len(output))]
        output=padding
        
    return output

  def decode(self,Tokens):                                                      #토큰은 정수화된 단어
    sentence=[]
    for token in Tokens:
      token=self.sp.DecodeIds(token)
      sentence.append(token)


    return sentence  

  def __len__(self):
    return len(self.sp)


def main():

  args=get_argument()
  tokenizer=SentencePieceTokenizer(args.datapath,vocab_size=args.vocab_size)    #1.tokenizer.py를 실행하여,template를 만드는 과정.template은 tokenizer모델의 hyperparmeters. (여기서 datapath는 토크나이저 모델 어디 저장할지만 결정)
  tokenizer.fit(",".join([args.srcpath,args.trgpath]))                          #2.위에서 만든 템플릿으로 영어파일, 독어파일로 tokenizer train한다.


if __name__ =="__main__":
  main()

