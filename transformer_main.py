# -*- coding: utf-8 -*-
"""transformer_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qxGHT1YIHahlwjBHpgPqwGwETjtB1r_3
"""

import numpy as np
import torch 
from transformers import make_model
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tokenizer import SentencePieceTokenizer
from Preprocess import CustomDataset
import argparse
from tqdm import tqdm
import pdb
import logging
import random
import torch.nn as nn

from tensorboardX import SummaryWriter
summary = SummaryWriter()

logging.basicConfig(format='%(asctime)s -  %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)
logger = logging.getLogger(__name__)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()

def get_argument():
  parser=argparse.ArgumentParser()

  parser.add_argument('--mode', default='train')
  parser.add_argument('--datapath',default='./datasets/iwslt14.de.en')
  parser.add_argument('--epoch',default=100,type=int)
  parser.add_argument('--learning_rate',default=0.0001,type=float)                         #type들을 지정해줘야함
  parser.add_argument('--max_seq_len',default=50,type=int)
  parser.add_argument('--langpair', default='de-en')
  parser.add_argument('--model_name',default='model')
  parser.add_argument('--batch',default=64, type=int)
  parser.add_argument('--seed',default=0, type=int)

  """evaluation, tokenization 나중에"""
  parser.add_argument('--eval_in')
  parser.add_argument('--eval_out')  


  args=parser.parse_args()
  logger.info(f"device: {device}, n_gpu: {n_gpu}")
  random.seed(args.seed)
  np.random.seed(args.seed)
  torch.manual_seed(args.seed)
  if n_gpu > 0:
      torch.cuda.manual_seed_all(args.seed)


  return args





def main(args):
  if args.mode == 'train':
    tokenizer=SentencePieceTokenizer(args.datapath,alpha=0,l=0).loadmodel()
    train_dataset=CustomDataset(tokenizer, args.max_seq_len, args.datapath, args.langpair, type='train')
    valid_dataset=CustomDataset(tokenizer, args.max_seq_len, args.datapath, args.langpair, type='valid')

    train_input=DataLoader(train_dataset,batch_size=args.batch, shuffle=True,collate_fn=train_dataset.collate_fn)    #dataloader는 데이터에 cuda기능도 함
    valid_input=DataLoader(valid_dataset,batch_size=args.batch, shuffle=True,collate_fn=valid_dataset.collate_fn)

    model=make_model(source_vocab=len(tokenizer), target_vocab=len(tokenizer), N=6,d_model=512, d_ff=2048, h=8, dropout=0.1).to(device)

    criterion=nn.NLLLoss(ignore_index=0)
    optimizer=optim.Adam(model.parameters(),lr=args.learning_rate)

    #best loss: 모델 저장하는 기준이 됨. best loss를 갱신하는 모델의 parameter들을 저장해줌
    best_loss = float("inf")
    train_global_step = 0
    val_global_step = 0
    cnt=0
    for epoch in range(args.epoch):
      train_loss=0
      valid_loss=0

      train_count=0
      valid_count=0

      model.train()                                                             #train 
      for i, data in tqdm(enumerate(train_input),total=len(train_input)):
        inputs, outputs=data
        targets=outputs
        bos_token=torch.ones(outputs.size()[0],1).long().cuda()*2
        outputs=torch.cat((bos_token,outputs),dim=-1)
        outputs=outputs[:,:-1]
        
        output_prob=model(inputs,False,outputs,True)                            #False, True는 model에서 바꾸고 수정하기
        loss=criterion(output_prob.view(-1,len(tokenizer)),targets.view(-1))    #len(tokenizer)는 vocab개수 

        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        train_count += 1
        if (train_global_step + 1) % 10 == 0:
                    summary.add_scalar('loss/loss_train', loss.item(), train_global_step) # tensorboard에 write
                    
        train_global_step +=1
      train_loss /= train_count

      model.eval()
      with torch.no_grad():
        for i, data in tqdm(enumerate(valid_input),total=len(valid_input)):
          inputs, outputs=data
          targets=outputs
          bos_token=torch.ones(outputs.size()[0],1).long().cuda()*2
          outputs=torch.cat((bos_token,outputs),dim=-1)
          outputs=outputs[:,:-1]
        
          output_prob=model(inputs,False,outputs,True)                            #False, True는 model에서 바꾸고 수정하기
          loss=criterion(output_prob.view(-1,len(tokenizer)),targets.view(-1))    #len(tokenizer)는 vocab개수 
          valid_loss += loss.item()*len(outputs)
          valid_count += len(outputs)
          if (val_global_step + 1) % 10 == 0:
                        summary.add_scalar('loss/loss_val', loss.item(), val_global_step) # tensorboard 

          val_global_step+=1
        valid_loss/=valid_count


      print("Epoch {}, Train_Loss: {:.3f}, Val_Loss: {:.3f}".format(epoch+1, train_loss, valid_loss))
      if best_loss > valid_loss:
          best_loss = valid_loss
          torch.save(model.state_dict(), "./outputs/{}3.pt".format(args.model_name))
          print("model saved!")
          cnt = 0 
      else:
          cnt += 1
            
  if args.mode == 'test':
    tokenizer=SentencePieceTokenizer(args.datapath).loadmodel()
    model=make_model(source_vocab=len(tokenizer), target_vocab=len(tokenizer), N=6,d_model=512, d_ff=2048, h=8, dropout=0.1).to(device)
    model.load_state_dict(torch.load("./outputs/{}3.pt".format(args.model_name)))
    model.eval()
    
    def translation(inputs):
          len_inputs=len(inputs)
          print(inputs)
          inputs=torch.tensor([tokenizer.Encode(input,50) for input in inputs]).cuda()
          outputs = torch.tensor([[2]]*len_inputs).cuda()

          for i in range(50):
            predicts=model(inputs,False,outputs,True)
            predicts=torch.argmax(predicts,dim=-1)[:,-1] 
            outputs=torch.cat((outputs,predicts.view(-1,1)),dim=-1)

          outputs=outputs.tolist()
          result_outputs=[]

          for i in outputs:
            try:
                eos=i.index(3)
                i=i[:eos]
            except:
              pass  

            result_outputs.append(i)
          outputs=result_outputs
          return tokenizer.decode(outputs)



    f=open("{}".format(args.eval_in),mode='r',encoding='utf-8')
    inputs=f.readlines()
    
    batch_size=32
    outputs=[]
    for minibatch in tqdm([inputs[i:i + batch_size] for i in range(0, len(inputs), batch_size)]):
            outputs += translation(minibatch)
        
    
    f=open("{}".format(args.eval_out),mode='w',encoding='utf-8')
    f.write('\n'.join(outputs)+'\n')
    
       







if __name__=="__main__":
  main(get_argument())